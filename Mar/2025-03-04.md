# TIL Template

## 날짜: 2025-03-04

## Model Architecture(모델 구성)
- Model Architecture는 신경망에서 각 레이어의 구성, 연결 방식, 활성화 함수, 입력 및 출력 형태 등 전체 모델의 구조이다. 특히 딥러닝 모델에서 다양한 레이어가 어떻게 결합하여 데이터를 처리하고 결과를 도출하는지 정의하는 중요한 개념이다.

- 딥러닝 모델은 기본적으로 학습 과정을 통해 찾아낸 최적 weight들의 집합니다.

### Epoch
- Training Data와 Validation Data로 나눈 데이터로 학습을 한다고 가정하면 아래와 같은 과정으로 Epoch이 수행된다.
    1. Training Data마다 각각 다른 비선형 그래프가 그려지고 Learning Rate를 조정하여 학습을 진행하면서 최적의 Loss값을 찾는다.
    2. 모든 Training Data에 대한 학습이 완료되면 모델이 학습한 내용을 바탕으로 Validation Data에 대한 예측을 수행한다.
    3. Validation Data의 Label과 모델의 예측 값을 비교하여 Loss값과 Accuracy를 계산하여 해당 Epoch에 대한 학습과 검증 결과로 Loss와 Accuracy 값이 출력된 후 학습된 모델이 저장된다.

### Loss(손실)
- 실제 값과 예측 값의 차이로 $|Y - \hat Y|$
- Loss가 0에 가까울수록 좋다. 그러나 Loss가 낮다고 무조건 좋은 것은 아니며 Overfitting의 경우 Loss가 낮아도 테스트 데이터의 성능이 떨어질 수 있다.

### Accuracy(정확도)
- 전체 $n$개의 샘플에 대해 각 샘플 $i$에 대해 모델의 예측과 실제 값이 일치하는 정도</br>
    전체 샘플 : $\text{Accuracy} = \frac{1}{n}\sum_{i=1}^{n} \mathbf{1}\{y_i = \hat{y}_i\}$</br>
    이진 분류 : $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$

## Pre-trained Model(사전 훈련 모델)
- 사전 훈련 모델은 이미 훈련이 끝난 모델 또는 모델 구성요소를 말하며 이 모델은 대규모 데이터셋에서 미리 학습되어 특정한 작업에 필요한 특성을 이미 학습한 상태이다.

- 사전 훈련 모델을 사용하는 이유는 인공지능 모델을 처음부터 구축하지 않아도 시간과 자원을 절약하며 신속하게 활요할 수 있기 때문이다.

## ResNet(Residual Network)
- ResNet은 Neural Network에서 발생하는 기울기 소실 문제를 해결하기 위해 Residual Connections을 도입한 사전 훈련 모델이다.</br>
Residual Connections(Skip connection)을 통해 깊은 네트워크에서도 Vanishing Gradient가 현저히 줄어든 상태로 학습을 진행할 수 있으며 ResNet은 50, 101, 152층 등 다양한 깊이의 모델로 구성된다.

- ResNet은 매우 깊은 신경망에서도 기울기 소실 문제를 효과적으로 해결하여 더 높은 정확도와 성능을 제공하는 딥러닝 모델을 구축할 수 있다.

## VGG16
- Visual Geometry Group이라는 영국 옥스포드 대학교의 연구그룹이 제안한 CNN 구조로 13개 층의 Conv Layer와 3개의 Fully-Connected Layer를 합친 CNN 구조이다.

- VGG16은 깊고 구조화된 CNN 구조의 사전 학습 모델로 주로 이미지 인식과 분류 작업에 사용되는 사전 훈련 모델이다. VGG16은 심층 신경망의 설계를 단순화하면서도 강력한 성능을 유지하는데 초점을 맞춘 모델로 모든 합성곱 계층에서 3x3 크기의 필터를 사용하여 작은 수용영역으로도 복잡한 특징을 효과적으로 학습할 수 있도록 설계되었다.

- VGG16은 ImageNet 데이터셋으로 사전 훈련된 가중치를 제공하여 Transfer Learning에도 효과적으로 확용될 수 있다. 138M개의 파라미터를 가지는 대규모 모델이므로 연산량과 메모리 사용량이 크다는 단점이 있다.

## Transfer Learning
- Transfer Learning은 사전 훈련된 모델을 그대로 사용하거나 추가 튜닝하여 새로운 문제에 적용함으로써 학습 시간을 단축하고 성능을 향상시키는 머신 러닝 기법이다. Transfer Learning 기법을 사용하면 대규모 데이터셋 또는 특정 과제에서 사전 훈련된 네트워크가 추출해 낸 구수준 특성(Feature)이나 모델 가중치(Weight)를 새로운 과제에 적용함으로써 학습시간을 단축하고 모델 성능을 개선할 수 있게 된다.

- 전이 학습에는 주로 3가지 기법 (Fine-Tuning, Feature Extraction, Zero-Shot Learning)이 있다.

### Fine-tuning(미세 조정)
- 사전 훈련된 모델의 전체 혹은 일부 계층을 새로운 데이터셋에 맞게 재훈련하여 최적화하는 과정이다. 이미 학습된 가중치와 패턴을 활용하면서도 새로운 문제의 특성을 반영하기 위해 미세한 조정 과정을 거친다.

- 이미 학습된 가중치와 패턴을 활용하면서도 새로운 문제의 특성을 반영하기 위해 미세한 조정 과정을 거친다. 이 기법은 학습 자원이 많이 들 수 있지만 데이터셋 특유의 패턴을 보다 깊이 있게 반영하므로 더 높은 성능을 얻을 가능성이 크다.

1. Full Fine Tuning</br>
    모델의 모든 파라미터를 새로운 데이터셋에 맞추어 학습화는 과정이다. 모델이 새로운 작업에 완전히 적응하도록 하기 때문에 가장 높은 성능을 기대할 수 있으나 많은 데이터와 계산 자원이 필요할 수 있으며 학습 시간이 오래 걸릴 수 있다.
    - 단계</br>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 모든 레이어를 동결 해제 $\rightarrow$ 모델 컴파일 $\rightarrow$ 전체 모델 학습 $\rightarrow$ 모델 평가

2. Partial Fine Tuning</br>
    모델의 일부 파라미터만 조정하는 방법이다. 모델의 상위 층(layer)만 미세 조정하고 하위 층은 그대로 두는 방식이다. 이렇게 하면 학습 시간이 단축되고 과적합(Overfitting) 문제를 줄일 수 있다.
    - 단계</br>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 하위 레이어를 동결하고 상위 레이어만 동결 해제 $\rightarrow$ 모델 컴파일 $\rightarrow$ 일부 레이어 학습 $\rightarrow$ 모델 평가

3. 단계적 Fine Tuning </br>
    먼저 모델의 상위 레이어만 재학습하고 그 다음 단계에서 하위 레이어까지 포함하여 재학습하는 접근법이다. 점진적으로 모델을 최적화하는데 사용된다.
    - 단계</br>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 기본 레이어를 동결 $\rightarrow$ 모델 컴파일 $\rightarrow$ 상위 레이어 학습 $\rightarrow$ 하위 레이어 동결 해제 및 상위 레이어 일부 동결 유지 $\rightarrow$ 모델 재컴파일 및 전체 모델 학습 $\rightarrow$ 모델 평가

4. 하이브리드 방법</br>
    여러 Fine Tuning 방법을 결합하여 사용하는 것으로 예를 들어 일부 레이어 Fine Tuning과 단계적 Fine Tuning을 결합하여 모델을 최적화한다.
    - 단계</br>
        사전 학습된 모델 불러오기 $\rightarrow$ 모델의 구조 변경 (새로운 분류 레이어 추가) $\rightarrow$ 하위 레이어를 동결하고 상위 레이어만 동결 해제 $\rightarrow$ 모델 컴파일 $\rightarrow$ 상위 레이어 학습 $\rightarrow$ 일부 하위 레이어 동결 해제 $\rightarrow$ 모델 재컴파일 및 전체 모델 학습 $\rightarrow$ 모델 평가

### Feature Extraction(특징 추출)
- 사전 훈련된 모델이 지닌 중간 계층의 가중치를 그대로 고정하여 특징 추출기(Feature Extractor)로 활용하는 방식이다. 그 후 마지막 분류 레이어만 새로 학습한다. 사전 훈련된 모델이 가진 광범위한 저수준 ~ 고수준 특징을 재활용하므로 데이터셋 규모가 크지 않아도 빠른 시간 내에 괜찮은 성능을 낼 수 있다.

- 새로운 문제와 사전 훈련 모델이 학습된 주제(도메인)가 유사할수록 특징 추출의 효율이 높다.

### Zero-Shot Learning(제로샷 학습)
- 학습에 한 번도 등장하지 않은 클래스에 대해서도 모델이 예측을 수행할 수 있도록 하는 전이 학습 기법니다. 모델이 사전에 개념(Concept) 정보나 멀티모달(텍스트$\cdot$이미지) 표현을 학습해두고 이후 새로운 클래스가 등장했을 때, 유사도나 개념 연결로 그 클래스를 식별한다.

- 데이터가 없는 클래스에 대해서도 추론할 수 있어 확장성이 높고 희귀한 상황이나 긴급상황 분류 등에 활용될 수 있다.


### 오늘의 도전 과제와 해결 방법
- 도전 과제 1: 도전 과제에 대한 설명 및 해결 방법
- 도전 과제 2: 도전 과제에 대한 설명 및 해결 방법

### 오늘의 회고
- 오늘의 학습 경험에 대한 자유로운 생각이나 느낀 점을 기록합니다.
- 성공적인 점, 개선해야 할 점, 새롭게 시도하고 싶은 방법 등을 포함할 수 있습니다.

### 참고 자료 및 링크
- [링크 제목](URL)
- [링크 제목](URL)
